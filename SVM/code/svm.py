# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rrs1jAl4Zz4I1TavY-kN0p0WlFeD0N2j
"""

import random
import matplotlib.pyplot as plt
import numpy as np
dataset=[(1,random.uniform(1.5,1.8),random.uniform(85,125)) for i in range(150) ]
dataset+=[(-1,random.uniform(1.7,1.9),random.uniform(40,70)) for i in range(90) ]
dataset+= [(-1,1.5,44.0),(-1,1.6,51.0),(-1,1.6,45.0),(-1,1.65,52.0),(-1,1.7,50.0)]
y=np.array( [ i[0]  for i in dataset ])
#y est vecteur colonne contenant les etiquettes {1,-1}

X=np.array( [ i[1:]  for i in dataset ]) 
#X est une matrice de Nindividu lignes et 2 colonnes (L,T) 

colors=np.array(["red" if i ==1 else "blue" for i in y ])
plt.scatter(X[:, 0], X[:, 1],  marker='o', c=colors)

import random
import matplotlib.pyplot as plt
import numpy as np



class mySVM:
    def __init__(self, pas=0.001, lamda=0.0001, N=10000):
        self.lr = pas
        self.lamda = lamda
        self.N = N         #nbre d'iterations
        self.w = None 
        self.b = None

    def fit(self, X, y):
        Nindividus, Nparametres = X.shape
        self.w = np.zeros(Nparametres)
        self.b = 0
        #initialisation
        
        for _ in range(self.N):
            for idx, x_i in enumerate(X):
                condition = y[idx] * (np.dot(x_i, self.w) - self.b) >= 1
                if condition:
                    self.w -= self.lr * (2 * self.lamda * self.w)
                else:
                    self.w -= self.lr * (
                        2 * self.lamda * self.w - np.dot(x_i, y[idx])
                    )
                    self.b -= self.lr * y[idx]
    #Cette partie de code permet de calculer le gradient selon les conditions (2.3)et(2.5)
    # et l'evaluer pour chaque w(k) afin de  calculer w(k+1)
    
    def predict(self, X):
        g = np.dot(X, self.w) - self.b
        return np.sign(g)
    #c'est l'implementation de l'equation (1.8)

donnee = mySVM()
donnee.fit(X, y)
#predictions = donnee.predict(X)
print(donnee.w, donnee.b)

def visualize_svm():
    def hyperplan(x, w, b, decalage):
        return (-w[0] * x + b + decalage) / w[1]
    # calcul de l'equation de l'hyperplan
    
    fig = plt.figure()
    a = fig.add_subplot(1, 1, 1)
    plt.scatter(X[:, 0], X[:, 1],  marker='o', c=colors)
    x0_1 = np.amin(X[:, 0])
    x0_2 = np.amax(X[:, 0])

    x1_1 = hyperplan(x0_1, donnee.w, donnee.b, 0)
    x1_2 = hyperplan(x0_2, donnee.w, donnee.b, 0)

    x1_1_m = hyperplan(x0_1, donnee.w, donnee.b, -1)
    x1_2_m = hyperplan(x0_2, donnee.w, donnee.b, -1)

    x1_1_p = hyperplan(x0_1, donnee.w, donnee.b, 1)
    x1_2_p = hyperplan(x0_2, donnee.w, donnee.b, 1)
    
    #Les points x0_1, x0_2, x1_1 , x1_2, x1_1_m , x1_2_m , x1_1_p et x1_2_p  permetteront 
    #de dessiner les droites des SVM et la droite de l'hyperplan median.
    
    
    a.plot([x0_1, x0_2], [x1_1, x1_2], "y")
    a.plot([x0_1, x0_2], [x1_1_m, x1_2_m], "g--")
    a.plot([x0_1, x0_2], [x1_1_p, x1_2_p], "g--")
    L=np.array([(1.6,80),(1.7,80),(1.6,55),(1.9,80),(1.77,100),(1.76,82),(1.7,110)])
    color=np.array(["purple" for i in L ])
    for x in L:
      print(donnee.predict(x))
    plt.scatter(L[:, 0], L[:, 1],  marker='o', c=color) 
    plt.show()

visualize_svm()

L=np.array([(1.6,80),(1.7,80),(1.6,55),(1.9,80),(2.26,100),(1.76,82),(1.7,110)])
color=np.array(["purple" for i in L ])
for x in L:
  print(donnee.predict(x))
plt.scatter(L[:, 0], L[:, 1],  marker='o', c=color)